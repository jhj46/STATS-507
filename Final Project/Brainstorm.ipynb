{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": true,
    "id": "ck4jLznJiooB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38b4cbfd496d80940cfa87c97061e3be",
     "grade": false,
     "grade_id": "cell-b1db85bf05a65a79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Brainstorming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outline.\n",
    "# Use this dataset to refine the other models with. \n",
    "    ## https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment\n",
    "        ### This model has training and testing split of 9.54K and 2.39k\n",
    "\n",
    "# Models\n",
    "## BERT style one called distilbert-base-uncased\n",
    "    ### https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "    ### https://huggingface.co/docs/transformers/model_doc/distilbert\n",
    "    ### This is faster than BERT and is a distilled version that doesn't care about letter case.\n",
    "## LSTM architecture?\n",
    "    ### https://keras.io/examples/nlp/bidirectional_lstm_imdb/ from the Huggingface Model https://huggingface.co/keras-io/bidirectional-lstm-imdb\n",
    "## Text Preprocessing Turns out this is just a think for prepping the data for the model.\n",
    "## Deep Learning This is the actual model that will be used to predict the sentiment of the tweets. It is the BERT model.\n",
    "## Sentiment Lexicon a possible model if I can find it.\n",
    "\n",
    "# 1 Import all the packages and libraries needed\n",
    "# 2 Import the Dataset.\n",
    "# 2.1 Tokenize and Pre-process data.\n",
    "# 3 Train the model.\n",
    "# 3.1 LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 Importing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.11.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed fsspec-2024.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.26.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: git-lfs in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.26.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2.5.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->transformers[torch]) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->transformers[torch]) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jajung\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# Checking system and getting libraries imported.\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "%pip install datasets \n",
    "%pip install transformers \n",
    "%pip install huggingface_hub\n",
    "%pip install git-lfs\n",
    "%pip install evaluate\n",
    "\n",
    "%pip install transformers[torch]\n",
    "%pip install tensorflow==2.12\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": true,
    "id": "7e5PDMTdlnhr",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "028c604f7c4a6b0fd489eb9063fb71bd",
     "grade": false,
     "grade_id": "cell-473f900194560999",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Section 2 Loading Data In. And Minor Data Exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14253fddb1644d50b682b93ee196a54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Loading Dataset\n",
    "\n",
    "\"\"\" \n",
    "import pandas as pd\n",
    "# The splits are test and validation\n",
    "splits = {'test': 'sent_train.csv', 'validation': 'sent_valid.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/zeroshot/twitter-financial-news-sentiment/\" + splits[\"validation\"])\n",
    "df.to_csv('zeroshotTwitterFinancialNewsSentiment_validation.csv', index=False) \n",
    "summary_stats = df.describe()\n",
    "print(summary_stats)\n",
    "value_counts = df['label'].value_counts()\n",
    "print(value_counts)\n",
    "summary_stats2 = df.describe(include=['object'])\n",
    "print(summary_stats2)\n",
    "\n",
    "\n",
    "# Testing combining files.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "test_sample_size = 500\n",
    "train_sample_size = 5000\n",
    "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
    "# Break up the dataset into random small training and testing datasets to speed up training.\n",
    "small_train_dataset = ds[\"train\"].shuffle(seed=42).select([i for i in list(range(train_sample_size))])\n",
    "small_test_dataset = ds[\"validation\"].shuffle(seed=42).select([i for i in list(range(test_sample_size))])\n",
    "\n",
    "# Using a Tokenizer.\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# PreProcessing the data to match model input format.\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    " \n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Use data_collector to convert our samples to PyTorch tensors and concatenate them with the correct amount of padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 - Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Adding the Trainer API\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Metrics\n",
    "import numpy as np\n",
    "import evaluate\n",
    " \n",
    "def compute_metrics_distilBERT(eval_pred):\n",
    "   load_accuracy = evaluate.load(\"accuracy\")\n",
    "   load_f1 = evaluate.load(\"f1\")\n",
    "   load_precision = evaluate.load(\"precision\")\n",
    "   load_recall = evaluate.load(\"recall\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "   precision = load_precision.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
    "   recall = load_recall.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4231d32707594fa19452e3833e67061e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jajung\\AppData\\Local\\Temp\\ipykernel_32188\\766610669.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_distilBERT = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name_distilBERT = \"finetuning-sentiment_twitter_financial-model-3000-samples\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repo_name_distilBERT,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\", \n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer_distilBERT = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_distilBERT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dac75da844043888b785817ede9d3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5495, 'grad_norm': 10.91074275970459, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\n",
      "{'train_runtime': 932.9115, 'train_samples_per_second': 10.719, 'train_steps_per_second': 0.671, 'train_loss': 0.5132403510827988, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a086a721956d4924b60ae8942772e50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# 5000 Group\\n\\n{'loss': 0.5393, 'grad_norm': 6.590184211730957, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\\n{'train_runtime': 913.2194, 'train_samples_per_second': 10.95, 'train_steps_per_second': 0.685, 'train_loss': 0.5016673456746549, 'epoch': 2.0}\\n# Evaluation\\n{'eval_loss': 0.41483020782470703,\\n 'eval_accuracy': 0.852,\\n 'eval_f1': 0.7981622878571132,\\n 'eval_runtime': 13.0949,\\n 'eval_samples_per_second': 38.183,\\n 'eval_steps_per_second': 2.444,\\n 'epoch': 2.0}\\n\\n\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runs the trainer\n",
    "trainer_distilBERT.train()\n",
    "#Using the sample 3000 the output was TrainOutput(global_step=376, training_loss=0.5834738548765791, metrics={'train_runtime': 533.74, 'train_samples_per_second': 11.241, 'train_steps_per_second': 0.704, 'total_flos': 79862707177344.0, 'train_loss': 0.5834738548765791, 'epoch': 2.0})\n",
    "#'train_runtime': 533.74, 'train_samples_per_second': 11.241, 'train_steps_per_second': 0.704, 'train_loss': 0.5834738548765791, 'epoch': 2.0}\n",
    "\"\"\"\n",
    "Training samples: 3000\n",
    "Test samples: 300\n",
    "First 5 token lengths: [16, 21, 49, 13, 24]\n",
    "Max tokens: 67\n",
    "Min tokens: 3\n",
    "Average tokens: 27.61\n",
    "\n",
    "Training samples: 5000\n",
    "Test samples: 500\n",
    "First 5 token lengths: [16, 21, 49, 13, 24]\n",
    "Max tokens: 81\n",
    "Min tokens: 3\n",
    "Average tokens: 27.71\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate the Model\n",
    "trainer_distilBERT.evaluate()\n",
    "\"\"\"\n",
    "# 5000 Group\n",
    "\n",
    "{'loss': 0.5393, 'grad_norm': 6.590184211730957, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\n",
    "{'train_runtime': 913.2194, 'train_samples_per_second': 10.95, 'train_steps_per_second': 0.685, 'train_loss': 0.5016673456746549, 'epoch': 2.0}\n",
    "# Evaluation\n",
    "{'eval_loss': 0.41483020782470703,\n",
    " 'eval_accuracy': 0.852,\n",
    " 'eval_f1': 0.7981622878571132,\n",
    " 'eval_runtime': 13.0949,\n",
    " 'eval_samples_per_second': 38.183,\n",
    " 'eval_steps_per_second': 2.444,\n",
    " 'epoch': 2.0}\n",
    "\n",
    " Run 2 with new metrics\n",
    " {'loss': 0.5495, 'grad_norm': 10.91074275970459, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\n",
    "{'train_runtime': 932.9115, 'train_samples_per_second': 10.719, 'train_steps_per_second': 0.671, 'train_loss': 0.5132403510827988, 'epoch': 2.0}\n",
    "\n",
    "{'eval_loss': 0.39613568782806396,\n",
    " 'eval_accuracy': 0.846,\n",
    " 'eval_f1': 0.7948768635043145,\n",
    " 'eval_precision': 0.807798243894731,\n",
    " 'eval_recall': 0.7836489303130407,\n",
    " 'eval_runtime': 10.6217,\n",
    " 'eval_samples_per_second': 47.074,\n",
    " 'eval_steps_per_second': 3.013,\n",
    " 'epoch': 2.0}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee72f9c9e2bb494dbab1014d85c5bb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.39613568782806396,\n",
       " 'eval_accuracy': 0.846,\n",
       " 'eval_f1': 0.7948768635043145,\n",
       " 'eval_precision': 0.807798243894731,\n",
       " 'eval_recall': 0.7836489303130407,\n",
       " 'eval_runtime': 10.6217,\n",
       " 'eval_samples_per_second': 47.074,\n",
       " 'eval_steps_per_second': 3.013,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_distilBERT.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size_analysis():\n",
    "    print(f\"Training samples: {tokenized_train.num_rows}\")\n",
    "    print(f\"Test samples: {tokenized_test.num_rows}\")\n",
    "    # Check token lengths for the first 5 samples in the training dataset\n",
    "    token_lengths = [len(sample['input_ids']) for sample in tokenized_train]\n",
    "    print(f\"First 5 token lengths: {token_lengths[:5]}\")\n",
    "    import numpy as np\n",
    "\n",
    "    token_lengths = [len(sample['input_ids']) for sample in tokenized_train]\n",
    "    print(f\"Max tokens: {np.max(token_lengths)}\")\n",
    "    print(f\"Min tokens: {np.min(token_lengths)}\")\n",
    "    print(f\"Average tokens: {np.mean(token_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Architecture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Run 2 \\nEpoch 1/2\\n157/157 ━━━━━━━━━━━━━━━━━━━━ 22s 113ms/step - accuracy: 0.6480 - loss: 0.8689 - val_accuracy: 0.7420 - val_loss: 0.6316\\nEpoch 2/2\\n157/157 ━━━━━━━━━━━━━━━━━━━━ 17s 109ms/step - accuracy: 0.8179 - loss: 0.4405 - val_accuracy: 0.7860 - val_loss: 0.5896\\n'\\nEpoch 1/2\\n157/157 ━━━━━━━━━━━━━━━━━━━━ 24s 125ms/step - accuracy: 0.6591 - loss: 0.8557 - val_accuracy: 0.7300 - val_loss: 0.6333\\nEpoch 2/2\\n157/157\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM Model from https://keras.io/examples/nlp/bidirectional_lstm_imdb/ from the Huggingface Model https://huggingface.co/keras-io/bidirectional-lstm-imdb\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "LSTMmodel = keras.Model(inputs, outputs)\n",
    "#LSTMmodel.summary()\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# PreProcessing the data to match model input format.\n",
    "train_texts = small_train_dataset[\"text\"]\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_texts = small_test_dataset[\"text\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "#Tokenizing the data. Don't include test_texts in training due to data leakage but use OOV to handle unknown words.\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "# Padding the sequences to the same length as needed for LSTM.\n",
    "max_len = 200\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Adding more Metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "# Define the F1 score function with the metrics instantiated once.\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Convert predictions to class labels\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    # Instantiate Precision and Recall metrics outside the function\n",
    "    precision = metrics.Precision()\n",
    "    recall = metrics.Recall()\n",
    "    \n",
    "    precision_value = precision(y_true, y_pred)\n",
    "    recall_value = recall(y_true, y_pred)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * (precision_value * recall_value) / (precision_value + recall_value + tf.keras.backend.epsilon())\n",
    "    return f1\n",
    "\n",
    "#Train and Evaluate Model\n",
    "#LSTMmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#LSTMmodel.fit(train_padded, train_labels, batch_size=32, epochs=2, validation_data=(test_padded, test_labels))\n",
    "\n",
    "\n",
    "\"\"\" Run 1\n",
    "Epoch 1/2\n",
    "157/157 ━━━━━━━━━━━━━━━━━━━━ 24s 125ms/step - accuracy: 0.6591 - loss: 0.8557 - val_accuracy: 0.7300 - val_loss: 0.6333\n",
    "Epoch 2/2\n",
    "157/157 ━━━━━━━━━━━━━━━━━━━━ 20s 126ms/step - accuracy: 0.8031 - loss: 0.4664 - val_accuracy: 0.8040 - val_loss: 0.5666\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Run 2 \n",
    "Epoch 1/2\n",
    "157/157 ━━━━━━━━━━━━━━━━━━━━ 22s 113ms/step - accuracy: 0.6480 - loss: 0.8689 - val_accuracy: 0.7420 - val_loss: 0.6316\n",
    "Epoch 2/2\n",
    "157/157 ━━━━━━━━━━━━━━━━━━━━ 17s 109ms/step - accuracy: 0.8179 - loss: 0.4405 - val_accuracy: 0.7860 - val_loss: 0.5896\n",
    "'\\nEpoch 1/2\\n157/157 ━━━━━━━━━━━━━━━━━━━━ 24s 125ms/step - accuracy: 0.6591 - loss: 0.8557 - val_accuracy: 0.7300 - val_loss: 0.6333\\nEpoch 2/2\\n157/157\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "# 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#\n",
    "# Path to the model directory\n",
    "model_path = r\"C:\\Users\\Jajung\\.cache\\huggingface\\hub\\models--keras-io--bidirectional-lstm-imdb\\snapshots\\b747ad4fa29ee40fb6093799a4807a3a8d5fea2e\"\n",
    "# Load the model\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_DISABLE_SYMLINKS\"] = \"1\"\n",
    "#Import model\n",
    "import tensorflow as tf\n",
    "#Need to import Keras\n",
    "from huggingface_hub import from_pretrained_keras\n",
    "model = from_pretrained_keras(\"keras-io/bidirectional-lstm-imdb\")\n",
    "# PreProcessing the data to match model input format.\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_texts = small_train_dataset[\"text\"]\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_texts = small_test_dataset[\"text\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "#Tokenizing the data. Don't include test_texts in training due to data leakage but use OOV to handle unknown words.\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "# Padding the sequences to the same length as needed for LSTM.\n",
    "max_len = 100\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "# Convert to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_padded, train_labels)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_padded, test_labels)).batch(64)\n",
    "\n",
    "##### Training\n",
    "\n",
    "# Adding the Trainer API\n",
    "#model.fit(train_padded, train_labels, epochs=5, batch_size=32, validation_data=(test_padded, test_labels))\n",
    "from transformers import TFTrainer, TFTrainingArguments\n",
    "repo_name = \"LSTM_finetuning-sentiment_twitter_financial-model\"\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir=repo_name,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "trainer = TFTrainer(\n",
    "    model=LSTMmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "#trainer.train()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another LSTM Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at LYTinn/lstm-finetuning-sentiment-model-3000-samples and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 300]) in the checkpoint and torch.Size([3, 300]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Jajung\\AppData\\Local\\Temp\\ipykernel_32188\\729179695.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_LSTM = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" First Run.\\n{'eval_loss': 0.8863855600357056,\\n 'eval_accuracy': 0.652,\\n 'eval_f1': 0.26311541565778856,\\n 'eval_precision': 0.21733333333333335,\\n 'eval_recall': 0.3333333333333333,\\n 'eval_runtime': 2.8323,\\n 'eval_samples_per_second': 176.538,\\n 'eval_steps_per_second': 11.298,\\n 'epoch': 2.0}\\n \""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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#\n",
    "\"\"\"\n",
    "\n",
    "# Using a Tokenizer.\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LYTinn/lstm-finetuning-sentiment-model-3000-samples\")\n",
    "\n",
    "\n",
    "# PreProcessing the data to match model input format.\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    " \n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Use data_collector to convert our samples to PyTorch tensors and concatenate them with the correct amount of padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# Adding the Trainer API\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "LSTM2model = AutoModelForSequenceClassification.from_pretrained(\"LYTinn/lstm-finetuning-sentiment-model-3000-samples\",num_labels=3, ignore_mismatched_sizes=True)\n",
    "# Defining Metrics\n",
    "import numpy as np\n",
    "import evaluate\n",
    " \n",
    "def compute_metrics_LSTM(eval_pred):\n",
    "   load_accuracy = evaluate.load(\"accuracy\")\n",
    "   load_f1 = evaluate.load(\"f1\")\n",
    "   load_precision = evaluate.load(\"precision\")\n",
    "   load_recall = evaluate.load(\"recall\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "   precision = load_precision.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
    "   recall = load_recall.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "    \n",
    "   return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Training the model\n",
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name_LSTM = \"finetuning-sentiment_twitter_financial-LSTMmodel\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repo_name_LSTM,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\", \n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer_LSTM = Trainer(\n",
    "    model=LSTM2model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_LSTM,\n",
    ")\n",
    "\n",
    "#trainer_LSTM.train()\n",
    "\n",
    "\"\"\"\n",
    "{'loss': 0.8922, 'grad_norm': 0.7044676542282104, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\n",
    "{'train_runtime': 89.5358, 'train_samples_per_second': 111.687, 'train_steps_per_second': 6.992, 'train_loss': 0.8921709517701365, 'epoch': 2.0}\n",
    "\"\"\"\n",
    "\n",
    "#trainer_LSTM.evaluate()\n",
    "\n",
    "\"\"\" First Run.\n",
    "{'eval_loss': 0.8863855600357056,\n",
    " 'eval_accuracy': 0.652,\n",
    " 'eval_f1': 0.26311541565778856,\n",
    " 'eval_precision': 0.21733333333333335,\n",
    " 'eval_recall': 0.3333333333333333,\n",
    " 'eval_runtime': 2.8323,\n",
    " 'eval_samples_per_second': 176.538,\n",
    " 'eval_steps_per_second': 11.298,\n",
    " 'epoch': 2.0}\n",
    " \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding More Data and Testing Again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7a50b4407847a68bc79758bc62b3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dc786a5d4b47ce85b981e2ef885f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1813 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57fa5ddc4d5475193768d91759b1577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/9543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883e59325d634b90a19b58931028f28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/7242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da531bbec52e44d296dbb6bc0f5c24b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62975db6497342d299fc0b331948dd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1813 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16785\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4201\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "from datasets import Value\n",
    "# the Pandas dataframe to load in the dataset.\n",
    "splits = {'fiqa_2018': 'data/fiqa_2018-00000-of-00001-ea3e4fffb9862c66.parquet', 'financial_phrasebank': 'data/financial_phrasebank-00000-of-00001-46d5c1a7817abe3d.parquet', 'twitter_financial_news_sentiment': 'data/twitter_financial_news_sentiment-00000-of-00001-e4ff9b93e3f0bcb7.parquet', 'auditor_sentiment': 'data/auditor_sentiment-00000-of-00001-7c7834e29b677695.parquet'}\n",
    "df = {\n",
    "    key: pd.read_parquet(\"hf://datasets/RobertoMCA97/financial_sentiment_analysis_train_compilation/\" + path)\n",
    "    for key, path in splits.items()\n",
    "}\n",
    "# remove the twitter_financial_news_sentiment as it is in the other dataset that we will later merge with.\n",
    "df.pop('twitter_financial_news_sentiment', None)\n",
    "# Changes the dataframes into datasets.\n",
    "compdatasets = {key: Dataset.from_pandas(df) for key, df in df.items()}\n",
    "# Splits the sub datasets into training and testing datasets.\n",
    "split_datasets = {\n",
    "    key: dset.train_test_split(test_size=0.2) for key, dset in compdatasets.items()\n",
    "}\n",
    "# wraps the datasets into a DatasetDict\n",
    "final_dataset = DatasetDict({\n",
    "    key: DatasetDict({'train': split['train'], 'test': split['test']})\n",
    "    for key, split in split_datasets.items()\n",
    "})\n",
    "# Combine all the splits into one split for training and one for testing. \n",
    "combined_dataset = DatasetDict({\n",
    "    'train': concatenate_datasets([ds['train'] for ds in final_dataset.values()]),\n",
    "    'test': concatenate_datasets([ds['test'] for ds in final_dataset.values()])\n",
    "})\n",
    "\n",
    "# Need to fix the label for combined_dataset to be numbers instead of words postive, negative, neutral.\n",
    "label_mapping = {'negative': 0, 'positive': 1, 'neutral': 2}\n",
    "\n",
    "# Function to map the labels\n",
    "def map_labels(example):\n",
    "    example['label'] = label_mapping[example['label']]\n",
    "    return example\n",
    "\n",
    "# Apply the mapping function to the train and test splits\n",
    "combined_dataset[\"train\"] = combined_dataset[\"train\"].map(map_labels)\n",
    "combined_dataset[\"test\"] = combined_dataset[\"test\"].map(map_labels)\n",
    "\n",
    "# Now need to take the dataset from before ds and combine it with combined_dataset.\n",
    "# First need to make sure the type for the label is int.\n",
    "ds[\"train\"] = ds[\"train\"].cast_column(\"label\", Value(\"int64\"))\n",
    "combined_dataset[\"train\"] = combined_dataset[\"train\"].cast_column(\"label\", Value(\"int64\"))\n",
    "ds[\"validation\"] = ds[\"validation\"].cast_column(\"label\", Value(\"int64\"))\n",
    "combined_dataset[\"test\"] = combined_dataset[\"test\"].cast_column(\"label\", Value(\"int64\"))\n",
    "# Combine train splits\n",
    "train_combined = concatenate_datasets([ds[\"train\"], combined_dataset[\"train\"]])\n",
    "\n",
    "# Combine validation and test splits\n",
    "test_combined = concatenate_datasets([ds[\"validation\"], combined_dataset[\"test\"]])\n",
    "\n",
    "# Create the final unified dataset\n",
    "final_combined_dataset = DatasetDict({\n",
    "    \"train\": train_combined,\n",
    "    \"test\": test_combined\n",
    "})\n",
    "\n",
    "print(final_combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Test samples: 500\n",
      "First 5 token lengths: [29, 17, 5, 30, 47]\n",
      "Max tokens: 120\n",
      "Min tokens: 3\n",
      "Average tokens: 28.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9decb4c2ac2d4610abf1cb1b3ab58223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3383, 'grad_norm': 8.523422241210938, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\n",
      "{'train_runtime': 918.7732, 'train_samples_per_second': 10.884, 'train_steps_per_second': 0.681, 'train_loss': 0.3077717589113278, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e6c69ebf5e43d1a02e3771f92ef510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Break up the dataset\n",
    "small_train_dataset = final_combined_dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(train_sample_size))])\n",
    "small_test_dataset = final_combined_dataset[\"test\"].shuffle(seed=42).select([i for i in list(range(test_sample_size))])\n",
    "\n",
    "#Preprocessing for distilBERT\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "sample_size_analysis()\n",
    "\"\"\"\n",
    "Training samples: 5000\n",
    "Test samples: 500\n",
    "First 5 token lengths: [29, 17, 5, 30, 47]\n",
    "Max tokens: 120\n",
    "Min tokens: 3\n",
    "Average tokens: 28.71\n",
    "\"\"\"\n",
    "#Train and Evaluate of distilBERT\n",
    "trainer_distilBERT.train()\n",
    "\"\"\"\n",
    "{'loss': 0.3383, 'grad_norm': 8.523422241210938, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\n",
    "{'train_runtime': 918.7732, 'train_samples_per_second': 10.884, 'train_steps_per_second': 0.681, 'train_loss': 0.3077717589113278, 'epoch': 2.0}\n",
    "\"\"\"\n",
    "\n",
    "trainer_distilBERT.evaluate()\n",
    "\n",
    "\"\"\"\n",
    "{'eval_loss': 0.40673962235450745,\n",
    " 'eval_model_preparation_time': 0.002,\n",
    " 'eval_accuracy': 0.868,\n",
    " 'eval_f1': 0.8335878133462916,\n",
    " 'eval_precision': 0.8232909352114651,\n",
    " 'eval_recall': 0.8451796340420202,\n",
    " 'eval_runtime': 10.4166,\n",
    " 'eval_samples_per_second': 48.0,\n",
    " 'eval_steps_per_second': 3.072,\n",
    " 'epoch': 2.0}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Test samples: 500\n",
      "First 5 token lengths: [29, 17, 5, 30, 47]\n",
      "Max tokens: 120\n",
      "Min tokens: 3\n",
      "Average tokens: 28.71\n",
      "Epoch 1/2\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 108ms/step - accuracy: 0.6086 - loss: 0.9137 - val_accuracy: 0.7240 - val_loss: 0.7126\n",
      "Epoch 2/2\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 104ms/step - accuracy: 0.7851 - loss: 0.5195 - val_accuracy: 0.7600 - val_loss: 0.7079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2292c893980>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing for Keras LSTM\n",
    "train_texts = small_train_dataset[\"text\"]\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_texts = small_test_dataset[\"text\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "max_len = 200\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "sample_size_analysis()\n",
    "\n",
    "# Train and Evaluate Keras LSTM\n",
    "LSTMmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "LSTMmodel.fit(train_padded, train_labels, batch_size=32, epochs=2, validation_data=(test_padded, test_labels))\n",
    "\n",
    "\"\"\"\n",
    "Training samples: 5000\n",
    "Test samples: 500\n",
    "First 5 token lengths: [29, 17, 5, 30, 47]\n",
    "Max tokens: 120\n",
    "Min tokens: 3\n",
    "Average tokens: 28.71\n",
    "Epoch 1/2\n",
    "157/157 ━━━━━━━━━━━━━━━━━━━━ 22s 108ms/step - accuracy: 0.6086 - loss: 0.9137 - val_accuracy: 0.7240 - val_loss: 0.7126\n",
    "Epoch 2/2\n",
    "157/157 ━━━━━━━━━━━━━━━━━━━━ 16s 104ms/step - accuracy: 0.7851 - loss: 0.5195 - val_accuracy: 0.7600 - val_loss: 0.7079\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Test samples: 500\n",
      "First 5 token lengths: [29, 17, 5, 30, 47]\n",
      "Max tokens: 120\n",
      "Min tokens: 3\n",
      "Average tokens: 28.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32f6fb0f99a4afd8d2d83b98f195378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.931, 'grad_norm': 2.161088466644287, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\n",
      "{'train_runtime': 77.7723, 'train_samples_per_second': 128.581, 'train_steps_per_second': 8.049, 'train_loss': 0.9294520978348705, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0508f05d087b412782aeb9911dd1b822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jajung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8859131932258606,\n",
       " 'eval_accuracy': 0.628,\n",
       " 'eval_f1': 0.2571662571662572,\n",
       " 'eval_precision': 0.20933333333333334,\n",
       " 'eval_recall': 0.3333333333333333,\n",
       " 'eval_runtime': 2.278,\n",
       " 'eval_samples_per_second': 219.493,\n",
       " 'eval_steps_per_second': 14.048,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing for other LSTM\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LYTinn/lstm-finetuning-sentiment-model-3000-samples\")\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "sample_size_analysis()\n",
    "\"\"\"\n",
    "Training samples: 5000\n",
    "Test samples: 500\n",
    "First 5 token lengths: [29, 17, 5, 30, 47]\n",
    "Max tokens: 120\n",
    "Min tokens: 3\n",
    "Average tokens: 28.71\n",
    "\"\"\"\n",
    "\n",
    "#Train and Evaluate of other LSTM\n",
    "trainer_LSTM.train()\n",
    "\"\"\"\n",
    "{'loss': 0.931, 'grad_norm': 2.161088466644287, 'learning_rate': 4.02555910543131e-06, 'epoch': 1.6}\n",
    "{'train_runtime': 77.7723, 'train_samples_per_second': 128.581, 'train_steps_per_second': 8.049, 'train_loss': 0.9294520978348705, 'epoch': 2.0}\n",
    "\"\"\"\n",
    "trainer_LSTM.evaluate()\n",
    "\"\"\"\n",
    "{'eval_loss': 0.8859131932258606,\n",
    " 'eval_accuracy': 0.628,\n",
    " 'eval_f1': 0.2571662571662572,\n",
    " 'eval_precision': 0.20933333333333334,\n",
    " 'eval_recall': 0.3333333333333333,\n",
    " 'eval_runtime': 2.278,\n",
    " 'eval_samples_per_second': 219.493,\n",
    " 'eval_steps_per_second': 14.048,\n",
    " 'epoch': 2.0}\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture 2 practice Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
