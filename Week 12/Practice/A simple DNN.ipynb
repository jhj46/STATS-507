{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d0c8c3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Week 12 Practice:Will I pass Stats 507?\n",
    "\n",
    "Writing our first DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c4a92b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969f0f8a-f7b5-4939-9e6a-0c9feeab37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network class\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size1, hidden_size2, num_classes):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "\n",
    "        # 1st hidden layer\n",
    "        self.linear_1 = nn.Linear(num_features, hidden_size1)\n",
    "\n",
    "        # 2nd linear layer\n",
    "        self.linear_2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "\n",
    "        # output layer\n",
    "        self.linear_out = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        logits = self.linear_out(x)\n",
    "        probas = torch.sigmoid(logits)  # Using sigmoid for binary classification\n",
    "\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac4f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data for binary classification\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 2)  # 100 samples with 2 features\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary labels based on a simple condition\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Add an extra dimension for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee01c0eb-ec8d-480e-b6e1-0fc884a9711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_features = 2\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "num_classes = 1  # Binary classification\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c032d5fb-d357-41f4-9754-93ab4df6b7f7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.6881\n",
      "Epoch [200/10000], Loss: 0.6743\n",
      "Epoch [300/10000], Loss: 0.6607\n",
      "Epoch [400/10000], Loss: 0.6449\n",
      "Epoch [500/10000], Loss: 0.6252\n",
      "Epoch [600/10000], Loss: 0.6001\n",
      "Epoch [700/10000], Loss: 0.5692\n",
      "Epoch [800/10000], Loss: 0.5328\n",
      "Epoch [900/10000], Loss: 0.4919\n",
      "Epoch [1000/10000], Loss: 0.4484\n",
      "Epoch [1100/10000], Loss: 0.4049\n",
      "Epoch [1200/10000], Loss: 0.3640\n",
      "Epoch [1300/10000], Loss: 0.3270\n",
      "Epoch [1400/10000], Loss: 0.2946\n",
      "Epoch [1500/10000], Loss: 0.2667\n",
      "Epoch [1600/10000], Loss: 0.2430\n",
      "Epoch [1700/10000], Loss: 0.2226\n",
      "Epoch [1800/10000], Loss: 0.2051\n",
      "Epoch [1900/10000], Loss: 0.1900\n",
      "Epoch [2000/10000], Loss: 0.1773\n",
      "Epoch [2100/10000], Loss: 0.1663\n",
      "Epoch [2200/10000], Loss: 0.1568\n",
      "Epoch [2300/10000], Loss: 0.1485\n",
      "Epoch [2400/10000], Loss: 0.1413\n",
      "Epoch [2500/10000], Loss: 0.1349\n",
      "Epoch [2600/10000], Loss: 0.1291\n",
      "Epoch [2700/10000], Loss: 0.1240\n",
      "Epoch [2800/10000], Loss: 0.1194\n",
      "Epoch [2900/10000], Loss: 0.1153\n",
      "Epoch [3000/10000], Loss: 0.1115\n",
      "Epoch [3100/10000], Loss: 0.1080\n",
      "Epoch [3200/10000], Loss: 0.1049\n",
      "Epoch [3300/10000], Loss: 0.1019\n",
      "Epoch [3400/10000], Loss: 0.0993\n",
      "Epoch [3500/10000], Loss: 0.0968\n",
      "Epoch [3600/10000], Loss: 0.0945\n",
      "Epoch [3700/10000], Loss: 0.0923\n",
      "Epoch [3800/10000], Loss: 0.0903\n",
      "Epoch [3900/10000], Loss: 0.0884\n",
      "Epoch [4000/10000], Loss: 0.0866\n",
      "Epoch [4100/10000], Loss: 0.0849\n",
      "Epoch [4200/10000], Loss: 0.0833\n",
      "Epoch [4300/10000], Loss: 0.0818\n",
      "Epoch [4400/10000], Loss: 0.0804\n",
      "Epoch [4500/10000], Loss: 0.0790\n",
      "Epoch [4600/10000], Loss: 0.0777\n",
      "Epoch [4700/10000], Loss: 0.0765\n",
      "Epoch [4800/10000], Loss: 0.0753\n",
      "Epoch [4900/10000], Loss: 0.0742\n",
      "Epoch [5000/10000], Loss: 0.0731\n",
      "Epoch [5100/10000], Loss: 0.0721\n",
      "Epoch [5200/10000], Loss: 0.0711\n",
      "Epoch [5300/10000], Loss: 0.0701\n",
      "Epoch [5400/10000], Loss: 0.0692\n",
      "Epoch [5500/10000], Loss: 0.0683\n",
      "Epoch [5600/10000], Loss: 0.0675\n",
      "Epoch [5700/10000], Loss: 0.0666\n",
      "Epoch [5800/10000], Loss: 0.0658\n",
      "Epoch [5900/10000], Loss: 0.0650\n",
      "Epoch [6000/10000], Loss: 0.0643\n",
      "Epoch [6100/10000], Loss: 0.0636\n",
      "Epoch [6200/10000], Loss: 0.0629\n",
      "Epoch [6300/10000], Loss: 0.0622\n",
      "Epoch [6400/10000], Loss: 0.0615\n",
      "Epoch [6500/10000], Loss: 0.0609\n",
      "Epoch [6600/10000], Loss: 0.0602\n",
      "Epoch [6700/10000], Loss: 0.0596\n",
      "Epoch [6800/10000], Loss: 0.0590\n",
      "Epoch [6900/10000], Loss: 0.0584\n",
      "Epoch [7000/10000], Loss: 0.0579\n",
      "Epoch [7100/10000], Loss: 0.0573\n",
      "Epoch [7200/10000], Loss: 0.0568\n",
      "Epoch [7300/10000], Loss: 0.0562\n",
      "Epoch [7400/10000], Loss: 0.0557\n",
      "Epoch [7500/10000], Loss: 0.0552\n",
      "Epoch [7600/10000], Loss: 0.0547\n",
      "Epoch [7700/10000], Loss: 0.0542\n",
      "Epoch [7800/10000], Loss: 0.0537\n",
      "Epoch [7900/10000], Loss: 0.0533\n",
      "Epoch [8000/10000], Loss: 0.0528\n",
      "Epoch [8100/10000], Loss: 0.0524\n",
      "Epoch [8200/10000], Loss: 0.0520\n",
      "Epoch [8300/10000], Loss: 0.0515\n",
      "Epoch [8400/10000], Loss: 0.0511\n",
      "Epoch [8500/10000], Loss: 0.0507\n",
      "Epoch [8600/10000], Loss: 0.0503\n",
      "Epoch [8700/10000], Loss: 0.0499\n",
      "Epoch [8800/10000], Loss: 0.0495\n",
      "Epoch [8900/10000], Loss: 0.0491\n",
      "Epoch [9000/10000], Loss: 0.0488\n",
      "Epoch [9100/10000], Loss: 0.0484\n",
      "Epoch [9200/10000], Loss: 0.0480\n",
      "Epoch [9300/10000], Loss: 0.0477\n",
      "Epoch [9400/10000], Loss: 0.0473\n",
      "Epoch [9500/10000], Loss: 0.0470\n",
      "Epoch [9600/10000], Loss: 0.0466\n",
      "Epoch [9700/10000], Loss: 0.0463\n",
      "Epoch [9800/10000], Loss: 0.0460\n",
      "Epoch [9900/10000], Loss: 0.0457\n",
      "Epoch [10000/10000], Loss: 0.0453\n"
     ]
    }
   ],
   "source": [
    "# Step1: Create the neural network model\n",
    "# YOUR CODE HERE\n",
    "model = MultilayerPerceptron(num_features, hidden_size1, hidden_size2, num_classes)\n",
    "\n",
    "# Step 2: Define loss function (binary cross-entropy)\n",
    "# YOUR CODE HERE\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "# Step 3: Define an optimizer (SGD)\n",
    "# YOUR CODE HERE\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    logits, outputs = model(X)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_func(outputs, y)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    # YOUR CODE HERE\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19f46a8-9ee6-4ff0-a5c0-f17c786aebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for [0.7, 0.6]: True (0 represents You can pass this class, 1 represents You will fail this calss)\n"
     ]
    }
   ],
   "source": [
    "# Testing the trained model\n",
    "with torch.no_grad():\n",
    "    test_data = torch.tensor([[0.7, 0.6]], dtype=torch.float32)\n",
    "    _, prediction = model(test_data)\n",
    "    predicted_class = (prediction > 0.5).item()\n",
    "    \n",
    "print(f\"Prediction for [0.7, 0.6]: {predicted_class} (0 represents You can pass this class, 1 represents You will fail this calss)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d0534a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'model' is your PyTorch model\n",
    "torch.save(model.state_dict(), 'saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a7ccf9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jajung\\AppData\\Local\\Temp\\ipykernel_14568\\329983537.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_new.load_state_dict(torch.load('saved_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0893, -0.3779],\n",
       "        [-0.0348, -0.3010],\n",
       "        [-0.7574, -0.4235],\n",
       "        [-0.3483,  0.5651],\n",
       "        [ 0.6663, -0.1861],\n",
       "        [ 0.3995,  0.7811],\n",
       "        [ 0.9346,  0.6208],\n",
       "        [-0.6108, -0.0046],\n",
       "        [ 0.4490, -0.1293],\n",
       "        [ 0.5247, -0.0200],\n",
       "        [ 0.4518,  0.1321],\n",
       "        [-0.1001, -0.5955],\n",
       "        [-0.0942,  0.3166],\n",
       "        [-0.2633, -0.3012],\n",
       "        [-0.4984, -0.2529],\n",
       "        [ 0.6287,  0.8097],\n",
       "        [-0.6254, -0.4139],\n",
       "        [-0.3576, -0.2488],\n",
       "        [-0.3172, -0.1040],\n",
       "        [-0.2986,  0.4842],\n",
       "        [-0.6851, -0.9718],\n",
       "        [-0.0296,  0.5133],\n",
       "        [-0.2352,  0.5903],\n",
       "        [-0.2662,  0.1321],\n",
       "        [ 0.4152, -0.1549],\n",
       "        [ 0.4646, -0.1618],\n",
       "        [ 0.6712,  0.8424],\n",
       "        [ 0.2492, -0.3529],\n",
       "        [ 0.8284,  0.7397],\n",
       "        [-0.4744,  0.2467],\n",
       "        [ 0.1176,  0.2749],\n",
       "        [ 0.7785,  0.9866],\n",
       "        [-0.2390, -0.7017],\n",
       "        [ 0.3701,  0.4027],\n",
       "        [ 0.3631,  0.6096],\n",
       "        [-0.6428, -0.6621],\n",
       "        [ 0.1601,  0.6904],\n",
       "        [ 0.5485, -0.3068],\n",
       "        [ 0.1223,  0.6003],\n",
       "        [ 0.8301,  0.3869],\n",
       "        [-0.5009, -0.5426],\n",
       "        [-0.1575, -0.1396],\n",
       "        [-0.4145, -0.1498],\n",
       "        [ 0.4565,  0.4791],\n",
       "        [ 0.2031,  0.1615],\n",
       "        [-0.6373, -0.5636],\n",
       "        [-0.5105,  0.6917],\n",
       "        [ 0.6186, -0.2557],\n",
       "        [-0.0947, -0.5848],\n",
       "        [-0.3571,  0.3516],\n",
       "        [-0.4796,  0.3730],\n",
       "        [-0.4944, -0.6080],\n",
       "        [-0.2070,  0.1900],\n",
       "        [-0.7091, -0.6721],\n",
       "        [ 0.3514,  0.4109],\n",
       "        [-0.2226,  0.6907],\n",
       "        [-0.1542, -0.2729],\n",
       "        [-0.0869, -0.8403],\n",
       "        [-0.5705,  0.5406],\n",
       "        [ 0.2737,  0.5184],\n",
       "        [-0.7061,  0.4314],\n",
       "        [ 0.6112, -0.4700],\n",
       "        [ 0.7274,  0.6117],\n",
       "        [ 0.3101, -0.3971]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model state dictionary\n",
    "model_new = MultilayerPerceptron(num_features, hidden_size1, hidden_size2, num_classes)\n",
    "model_new.load_state_dict(torch.load('saved_model.pth'))\n",
    "model_new.linear_1.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
