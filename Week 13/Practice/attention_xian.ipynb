{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d0c8c3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Week 13 Practice: A simple RNN\n",
    "\n",
    "Write RNN from scratch. Finish the forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4a92b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3693f6a9-bc41-4620-aa15-1ae0b4c98310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 8])\n",
      "tensor([[[-0.5433, -0.5050,  0.4409, -0.7617, -1.2403, -0.0746, -0.6283,\n",
      "          -1.0576],\n",
      "         [-0.5391,  0.3608,  1.1843,  0.3850,  0.6423,  0.3401,  0.0428,\n",
      "          -1.4827],\n",
      "         [ 1.4827,  0.4520,  0.1143,  0.5077,  0.2364,  0.7920,  0.6191,\n",
      "           1.1689],\n",
      "         [-1.3448,  0.3344,  2.0104,  0.7628,  2.5714, -0.3169, -0.8335,\n",
      "           0.3047]],\n",
      "\n",
      "        [[-1.4552, -1.5145, -1.2081,  0.5568, -0.4187,  1.3677, -0.0801,\n",
      "          -0.7354],\n",
      "         [-2.1575, -0.6595, -1.5990,  1.4023, -0.0898, -1.4643,  0.7791,\n",
      "           0.4537],\n",
      "         [-0.0755,  1.0204,  1.1901,  0.6441,  0.1853, -1.2675,  0.2914,\n",
      "           0.1710],\n",
      "         [ 0.0329, -0.1213, -0.7920, -0.3056, -1.0159,  0.2477,  0.2326,\n",
      "          -0.5507]]])\n",
      "Output shape: torch.Size([2, 4, 8])\n",
      "tensor([[[-0.1493, -0.3819,  0.1333, -0.2392,  0.0547, -0.4591,  0.2525,\n",
      "          -0.6258],\n",
      "         [-0.1610, -0.3904,  0.1121, -0.2588,  0.0392, -0.4388,  0.2552,\n",
      "          -0.6050],\n",
      "         [ 0.0144, -0.2151,  0.1815, -0.0647,  0.2872, -0.5830,  0.2993,\n",
      "          -0.6562],\n",
      "         [-0.0480, -0.3113,  0.2486, -0.0988,  0.1817, -0.5738,  0.2702,\n",
      "          -0.6923]],\n",
      "\n",
      "        [[-0.8790, -0.6350, -0.3199, -0.0305,  0.0555, -0.1543,  0.2551,\n",
      "           0.1061],\n",
      "         [-1.0135, -0.7343, -0.3330,  0.0698,  0.0473, -0.0907,  0.2753,\n",
      "           0.2739],\n",
      "         [-1.0293, -0.7387, -0.3345,  0.0685,  0.0410, -0.0974,  0.2725,\n",
      "           0.2756],\n",
      "         [-0.9536, -0.6824, -0.3362,  0.0159,  0.0539, -0.1176,  0.2654,\n",
      "           0.1984]]], grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Simple single-head self-attention implementation\n",
    "        \n",
    "        Args:\n",
    "            embed_dim (int): Dimension of input embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Scaling factor\n",
    "        self.scaling = embed_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        # Project inputs to Q, K, V\n",
    "        q = self.q_proj(x)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.k_proj(x)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.v_proj(x)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # (batch_size, seq_len, seq_len) = (batch_size, seq_len, embed_dim) @ (batch_size, embed_dim, seq_len)\n",
    "        attention_scores = torch.bmm(q, k.transpose(1, 2)) * self.scaling\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute output\n",
    "        # (batch_size, seq_len, embed_dim) = (batch_size, seq_len, seq_len) @ (batch_size, seq_len, embed_dim)\n",
    "        output = torch.bmm(attention_weights, v)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    # Parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    embed_dim = 8\n",
    "    \n",
    "    # Create model\n",
    "    attention = SingleHeadSelfAttention(embed_dim)\n",
    "    \n",
    "    # Create random input\n",
    "    x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = attention(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(x)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80a5cc-bc05-43fd-a7c9-ecf3989de9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
